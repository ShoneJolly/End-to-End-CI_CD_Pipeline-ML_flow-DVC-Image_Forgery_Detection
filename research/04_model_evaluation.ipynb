{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521cf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ceeaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GoFreeLabTechnologies\\\\Internship projects\\\\CI_CD_Pipeline\\\\CI_CD_Pipeline-ML_flow-DVC\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017ffc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\GoFreeLabTechnologies\\\\Internship projects\\\\CI_CD_Pipeline\\\\CI_CD_Pipeline-ML_flow-DVC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b963fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    model_path: Path\n",
    "    model: str\n",
    "    load_data: Path\n",
    "    mlflow_uri: str\n",
    "    params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433f286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as ShoneJolly\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as ShoneJolly\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ShoneJolly/CI_CD_Pipeline-ML_flow-DVC\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"ShoneJolly/CI_CD_Pipeline-ML_flow-DVC\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ShoneJolly/CI_CD_Pipeline-ML_flow-DVC initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository ShoneJolly/CI_CD_Pipeline-ML_flow-DVC initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='ShoneJolly', repo_name='CI_CD_Pipeline-ML_flow-DVC', mlflow=True)\n",
    "\n",
    "# import mlflow\n",
    "# with mlflow.start_run():\n",
    "#   mlflow.log_param('parameter name', 'value')\n",
    "#   mlflow.log_metric('metric name', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5bf7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ImageForgeryDetection.constants import *\n",
    "from ImageForgeryDetection.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66becc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "    \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.trainer_evaluation\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            model_path= config.model_path,\n",
    "            model= config.model,\n",
    "            load_data= config.load_data,\n",
    "            mlflow_uri= config.mlflow_uri,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f5f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\cicdproject\\lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.6 when it was built against 1.14.5, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "from ImageForgeryDetection import logger\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from ImageForgeryDetection.utils.common import save_json\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.score = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads test data from joblib files specified in config.\"\"\"\n",
    "        logger.info(f\"Loading test data from {self.config.load_data}\")\n",
    "        try:\n",
    "            x_path = Path(self.config.load_data) / 'X_90.joblib'\n",
    "            y_path = Path(self.config.load_data) / 'y.joblib'\n",
    "            X = joblib.load(x_path)\n",
    "            y = joblib.load(y_path)\n",
    "            logger.info(f\"Loaded X with shape {X.shape} and y with shape {y.shape}\")\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def split_data(self, X, y):\n",
    "        \"\"\"Splits data into training and testing sets.\"\"\"\n",
    "        logger.info(\"Splitting data into train and test sets\")\n",
    "        try:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "            logger.info(f\"Test set: X_test shape {X_test.shape}, y_test shape {y_test.shape}\")\n",
    "            return X_test, y_test\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error splitting data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_data(self, X_test, y_test):\n",
    "        \"\"\"Reshapes test data for CNN input.\"\"\"\n",
    "        logger.info(\"Preprocessing test data\")\n",
    "        try:\n",
    "            X_test = X_test.reshape(X_test.shape[0], 128, 128, 3)\n",
    "            y_test = y_test.reshape(y_test.shape[0], 2)\n",
    "            logger.info(f\"Reshaped X_test to {X_test.shape}, y_test to {y_test.shape}\")\n",
    "            self.X_test, self.y_test = X_test, y_test\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing data: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        \"\"\"Loads the trained model.\"\"\"\n",
    "        logger.info(f\"Loading model from {path}\")\n",
    "        try:\n",
    "            return tf.keras.models.load_model(path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_test_generator(self):\n",
    "        \"\"\"Returns a Sequence generator for test data.\"\"\"\n",
    "        class TestGenerator(Sequence):\n",
    "            def __init__(self, X, y, batch_size, **kwargs):\n",
    "                super().__init__(**kwargs)  # Initialize Sequence base class\n",
    "                self.X = X\n",
    "                self.y = y\n",
    "                self.batch_size = batch_size\n",
    "                self.indexes = np.arange(len(self.X))\n",
    "\n",
    "            def __len__(self):\n",
    "                return int(np.floor(len(self.X) / self.batch_size))\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "                X = [self.X[k] for k in indexes]\n",
    "                y = [self.y[k] for k in indexes]\n",
    "                return np.array(X), np.array(y)\n",
    "\n",
    "        return TestGenerator(self.X_test, self.y_test, self.config.params['batch_size'])\n",
    "\n",
    "    def evaluation(self):\n",
    "        \"\"\"Evaluates the model and saves scores.\"\"\"\n",
    "        logger.info(\"Starting model evaluation\")\n",
    "        try:\n",
    "            # Load and preprocess data\n",
    "            X, y = self.load_data()\n",
    "            X_test, y_test = self.split_data(X, y)\n",
    "            self.preprocess_data(X_test, y_test)\n",
    "\n",
    "            # Load model\n",
    "            model_path = Path(self.config.model_path) / self.config.model\n",
    "            self.model = self.load_model(model_path)\n",
    "\n",
    "            # Create test generator\n",
    "            test_generator = self.get_test_generator()\n",
    "\n",
    "            # Evaluate model\n",
    "            logger.info(\"Evaluating model on test data\")\n",
    "            self.score = self.model.evaluate(\n",
    "                test_generator,\n",
    "                batch_size=self.config.params['batch_size'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            logger.info(f\"Evaluation scores: {self.score}\")\n",
    "\n",
    "            # Save scores\n",
    "            self.save_score()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_score(self):\n",
    "        \"\"\"Saves evaluation scores to a JSON file.\"\"\"\n",
    "        logger.info(\"Saving evaluation scores\")\n",
    "        try:\n",
    "            # Handle F1 score as a tensor array by computing mean\n",
    "            f1_score = self.score.get('f1_score', 0.0)\n",
    "            if isinstance(f1_score, tf.Tensor):\n",
    "                f1_score = np.mean(f1_score.numpy())\n",
    "            elif isinstance(f1_score, np.ndarray):\n",
    "                f1_score = np.mean(f1_score)\n",
    "\n",
    "            scores = {\n",
    "                \"loss\": float(self.score.get('loss', 0.0)),\n",
    "                \"accuracy\": float(self.score.get('accuracy', 0.0)),\n",
    "                \"precision\": float(self.score.get('precision', 0.0)),\n",
    "                \"recall\": float(self.score.get('recall', 0.0)),\n",
    "                \"f1_score\": float(f1_score)\n",
    "            }\n",
    "            save_json(path=Path(\"scores.json\"), data=scores)\n",
    "            logger.info(\"Scores saved successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving scores: {e}\")\n",
    "            raise\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        \"\"\"Logs parameters, metrics, and model to MLflow.\"\"\"\n",
    "        logger.info(\"Logging to MLflow\")\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "            tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "            logger.info(f\"MLflow tracking URI: {self.config.mlflow_uri}\")\n",
    "\n",
    "            with mlflow.start_run():\n",
    "                # Log parameters\n",
    "                mlflow.log_params(self.config.params)\n",
    "\n",
    "                # Handle F1 score for logging\n",
    "                f1_score = self.score.get('f1_score', 0.0)\n",
    "                if isinstance(f1_score, tf.Tensor):\n",
    "                    f1_score = np.mean(f1_score.numpy())\n",
    "                elif isinstance(f1_score, np.ndarray):\n",
    "                    f1_score = np.mean(f1_score)\n",
    "\n",
    "                # Log metrics\n",
    "                mlflow.log_metrics({\n",
    "                    \"loss\": float(self.score.get('loss', 0.0)),\n",
    "                    \"accuracy\": float(self.score.get('accuracy', 0.0)),\n",
    "                    \"precision\": float(self.score.get('precision', 0.0)),\n",
    "                    \"recall\": float(self.score.get('recall', 0.0)),\n",
    "                    \"f1_score\": float(f1_score)\n",
    "                })\n",
    "\n",
    "                # Save model to a temporary .keras file\n",
    "                with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "                    temp_model_path = os.path.join(tmpdirname, \"model.keras\")\n",
    "                    logger.info(f\"Saving model to temporary path: {temp_model_path}\")\n",
    "                    self.model.save(temp_model_path)\n",
    "                    if not os.path.exists(temp_model_path):\n",
    "                        raise FileNotFoundError(f\"Failed to save model at {temp_model_path}\")\n",
    "                    logger.info(f\"Verified model saved at {temp_model_path}, size: {os.path.getsize(temp_model_path)} bytes\")\n",
    "\n",
    "                    # Log model as artifact\n",
    "                    logger.info(\"Logging model as MLflow artifact\")\n",
    "                    mlflow.log_artifact(temp_model_path, artifact_path=\"model\")\n",
    "                    logger.info(\"Model logged to MLflow as artifact successfully\")\n",
    "\n",
    "                    # Register the model in MLflow Model Registry\n",
    "                    if tracking_url_type_store != \"file\":\n",
    "                        logger.info(\"Registering model in MLflow as ImageForgeryDetectionModel\")\n",
    "                        client = mlflow.tracking.MlflowClient()\n",
    "                        run_id = mlflow.active_run().info.run_id\n",
    "                        try:\n",
    "                            # Create or update model in registry\n",
    "                            result = client.create_model_version(\n",
    "                                name=\"ImageForgeryDetectionModel\",\n",
    "                                source=f\"{mlflow.get_artifact_uri('model')}\",\n",
    "                                run_id=run_id\n",
    "                            )\n",
    "                            logger.info(f\"Model registered as ImageForgeryDetectionModel, version {result.version}\")\n",
    "                        except mlflow.exceptions.RestException as e:\n",
    "                            logger.error(f\"Failed to register model: {e.json}, Status Code: {e.status_code}\")\n",
    "                            raise\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Unexpected error during model registration: {str(e)}\")\n",
    "                            raise\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging to MLflow: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cec3611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-09 19:49:18,689: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-08-09 19:49:18,769: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-08-09 19:49:18,769: INFO: 373372311: Starting model evaluation]\n",
      "[2025-08-09 19:49:18,777: INFO: 373372311: Loading test data from artifacts/data_preprocessing/pickle]\n",
      "[2025-08-09 19:49:31,665: INFO: 373372311: Loaded X with shape (9501, 49152) and y with shape (9501, 2)]\n",
      "[2025-08-09 19:49:31,667: INFO: 373372311: Splitting data into train and test sets]\n",
      "[2025-08-09 19:49:36,887: INFO: 373372311: Test set: X_test shape (1901, 49152), y_test shape (1901, 2)]\n",
      "[2025-08-09 19:49:37,237: INFO: 373372311: Preprocessing test data]\n",
      "[2025-08-09 19:49:37,241: INFO: 373372311: Reshaped X_test to (1901, 128, 128, 3), y_test to (1901, 2)]\n",
      "[2025-08-09 19:49:37,258: INFO: 373372311: Loading model from artifacts\\model_trainer\\model\\model.keras]\n",
      "[2025-08-09 19:49:38,713: INFO: 373372311: Evaluating model on test data]\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 68ms/step - accuracy: 0.9237 - f1_score: 0.8812 - loss: 0.1982 - precision: 0.9237 - recall: 0.9237\n",
      "[2025-08-09 19:49:46,753: INFO: 373372311: Evaluation scores: {'accuracy': 0.9237288236618042, 'f1_score': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.95228624, 0.81002635], dtype=float32)>, 'loss': 0.19818778336048126, 'precision': 0.9237288236618042, 'recall': 0.9237288236618042}]\n",
      "[2025-08-09 19:49:46,755: INFO: 373372311: Saving evaluation scores]\n",
      "[2025-08-09 19:49:46,780: INFO: common: json file saved at: scores.json]\n",
      "[2025-08-09 19:49:46,786: INFO: 373372311: Scores saved successfully]\n",
      "[2025-08-09 19:49:47,117: INFO: 373372311: Logging to MLflow]\n",
      "[2025-08-09 19:49:47,119: INFO: 373372311: MLflow tracking URI: https://dagshub.com/ShoneJolly/CI_CD_Pipeline-ML_flow-DVC.mlflow]\n",
      "[2025-08-09 19:49:49,958: INFO: 373372311: Saving model to temporary path: C:\\Users\\hp\\AppData\\Local\\Temp\\tmp8mf27uix\\model.keras]\n",
      "[2025-08-09 19:49:50,339: INFO: 373372311: Verified model saved at C:\\Users\\hp\\AppData\\Local\\Temp\\tmp8mf27uix\\model.keras, size: 39256344 bytes]\n",
      "[2025-08-09 19:49:50,339: INFO: 373372311: Logging model as MLflow artifact]\n",
      "[2025-08-09 19:50:44,121: INFO: 373372311: Model logged to MLflow as artifact successfully]\n",
      "[2025-08-09 19:50:44,121: INFO: 373372311: Registering model in MLflow as ImageForgeryDetectionModel]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/09 19:50:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ImageForgeryDetectionModel, version 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-09 19:50:45,354: INFO: 373372311: Model registered as ImageForgeryDetectionModel, version 5]\n",
      "üèÉ View run intrigued-robin-655 at: https://dagshub.com/ShoneJolly/CI_CD_Pipeline-ML_flow-DVC.mlflow/#/experiments/0/runs/517a7bb8e7d6416c86dc3e25ac000ae6\n",
      "üß™ View experiment at: https://dagshub.com/ShoneJolly/CI_CD_Pipeline-ML_flow-DVC.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        config = ConfigurationManager()\n",
    "        eval_config = config.get_model_evaluation_config()\n",
    "        evaluation = ModelEvaluation(eval_config)\n",
    "        evaluation.evaluation()\n",
    "        evaluation.log_into_mlflow()\n",
    "except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cicdproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
